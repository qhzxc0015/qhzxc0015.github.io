{"meta":{"title":"thk_days","subtitle":"I think that that that that that article wrote on the blog was wrong.","description":"程序猿 | 熬夜猫 | 次元狗","author":"zxc","url":"http://qhzxc0015.com"},"pages":[{"title":"About","date":"2017-10-10T16:00:00.000Z","updated":"2017-10-12T07:00:43.347Z","comments":true,"path":"about/index.html","permalink":"http://qhzxc0015.com/about/index.html","excerpt":"","text":"hello world Pika Pika ~ Pikachu ~ is here. If Click Ghost , emmm... If you are single person (Oh,No)!, Please Click this or this or or this! Ok,ok~ haha ~ pr pr pr ~"},{"title":"guestbook","date":"2017-08-06T18:22:45.000Z","updated":"2017-08-07T10:33:37.106Z","comments":true,"path":"guestbook/index.html","permalink":"http://qhzxc0015.com/guestbook/index.html","excerpt":"","text":""},{"title":"categories","date":"2017-08-06T21:44:46.000Z","updated":"2017-08-06T21:52:18.927Z","comments":true,"path":"categories/index.html","permalink":"http://qhzxc0015.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-07-21T12:44:19.000Z","updated":"2017-07-21T12:56:21.358Z","comments":true,"path":"tags/index.html","permalink":"http://qhzxc0015.com/tags/index.html","excerpt":"","text":""},{"title":"photos","date":"2017-08-07T01:36:11.000Z","updated":"2017-11-22T16:13:35.709Z","comments":true,"path":"photos/index.html","permalink":"http://qhzxc0015.com/photos/index.html","excerpt":"","text":"$(\"#Thplayer\").thplayer({ title: \"稻香\", author: \"周杰伦\", cover: \"http://p4.music.126.net/gYwk-n_UWAtOfDZBEV04dQ==/7699879929738059.jpg\", music: \"http://ou9i51fe5.bkt.clouddn.com/music/%E7%A8%BB%E9%A6%99.mp3\" }); Welcom 你好啊，欢迎来到小世界， 这里很小，让我带你逛逛。 跟我走吧 那段奋不顾身的日子，叫青春 一直觉得自己的成长是一瞬间的。没有漫长的打坐，也没有光影明灭的交替， 忽然有那么一天，就被时光拽到了成人的世界，至此，泾渭分明。 喜欢你已超过两分钟，不能撤回 要多活一些岁月才知道，跟某些人之间永远没法斩钉截铁画下一个句号。 像少年一样无赖，像中年一样去爱 时间从未走远，在那些始终和时间赛跑的人身上。 每一天落寞的阳光变成防腐剂，每一日沉睡的星空覆上保护膜，任何外界的干扰，都成为独自前进的坚定力量。而这种力量，历久弥新，终于成为你生命中不可或缺的一部分。 我们拼命变好，是因为心里住着不想辜负的人 ... One L Details Two O Details Three V Details Four E Details 故事 还记得让你突然感到非常舒服的那些时刻吗？ 1. 高中晚自习，突然停电了。老师出了教室查看，顿时大家像起了化学反应，一下子从安静变的沸腾。等到老师回来沮丧的说，一时修不好，晚自习不上了。整个楼顿时传来尖叫声。 2. 雨淅淅沥沥的下着，周末在家一天没有洗脸，上午打扫房间洗衣服，到了中午叫了外卖吃了饱饱的午餐，困意上来。在干净的小窝里，舒心的听着雨打芭蕉般的声音，缓缓睡去，心宁静的仿佛世界温柔的和你一起静谧下去。 3. 忙碌了一整年，拉着行李回到家。敲开门，先是妈妈大大的拥抱，看到门口已经准备好我的专属拖鞋。洗手换上家里干净舒适的睡衣，餐厅一大桌子自己爱吃的菜。舒舒服服的大快朵颐一番，妈妈在旁边笑着看着我，新闻联播的序幕曲从客厅里传来。 4. 喜欢上一个人，她笑起来非常温柔可爱。找各种理由见面，内心很兴奋。一起看电影，心里砰砰跳个不停，她的手就在旁边，悄悄伸手拉住，没有任何言语却紧张到僵硬的像个雕塑凝固在那。 5. 夏天午觉刚睡醒，目光呆滞，智商减半，但内心深处知道，自己将要满血复活。于是坐在那里，看自己一点点的变身。 6. 今天是周五。 Twitter Facebook Instagram LinkedIn Email"},{"title":"project","date":"2017-08-07T02:21:43.000Z","updated":"2017-08-07T17:08:27.067Z","comments":true,"path":"project/index.html","permalink":"http://qhzxc0015.com/project/index.html","excerpt":"","text":""}],"posts":[{"title":"Scheduler","slug":"11. scheduler","date":"2018-01-29T20:50:15.000Z","updated":"2018-01-29T21:00:07.739Z","comments":true,"path":"2018/01/30/11. scheduler.html","link":"","permalink":"http://qhzxc0015.com/2018/01/30/11. scheduler.html","excerpt":"scheduler是kubernetes中独特而又重要的一个模块。独特是因为在kubernetes体系中，scheduler是唯一一个以plugin形式存在的模块，重要是因为kubernetes的基础单元pod的部署是通过scheduler完成的","text":"scheduler是kubernetes中独特而又重要的一个模块。独特是因为在kubernetes体系中，scheduler是唯一一个以plugin形式存在的模块，重要是因为kubernetes的基础单元pod的部署是通过scheduler完成的 1.目录结构kubernetes调度器的源码位于kubernetes/plugin/中，大体的代码目录结构如下所示：12345678910—-kubernetes——–plugin————cmd //kub-scheduler启动函数在cmd包中————pkg //调度相关的具体实现—————-scheduler——————–algorithm————————predicates //主机筛选策略————————priorities //主机打分策略——————–algorithmprovider————————defaults //定义默认的调度器 2. 调度过程 调度pod请求 待调度pod自身有namespace，pod名等信息 k8s集群开始遍历，找出所有可以被调度的节点生成nodelist（node1，node2…）并对nodelist的节点进行打分选出得分最高的node，此时生成node+待调度的pod信息的bingding对象 k8s将bingding对象发送给apiserver，apiserver将bingding对象信息发送给etcd进行持久化存储 etcd收到apiserver发送的bingding对象信息后，存储着node上所有运行pod信息对象boundpounds被更新 kubelet监测到etcd的boundpounds变化后根据变化向本地docker发送创建容器请求。 3. 关于pod分配 当新增一个容器时，集群会在可用的集群节点中寻找最合适的节点来运行相应的容器。 首先，集群会排出如下节点： 节点状态为不可用的“如节点不通或者k8s服务运行异常等”; 节点剩余的CPU,内存资源不足以运行容器的； 容器运行时占用的宿主机端口出现冲突的； 按照节点选择label不匹配的； 在排除不符合的节点之后，剩下的节点均为候选节点。容器具体调度到集群的哪台宿主机上，由调度器的积分机制决定。 例如节点A的打分将由如下公式决定：finalScoreNodeA = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + ……这里，有不同的评价策略以及其权重。每个节点获得的分值为节点按照各个评价策略及权重加和的值。 默认的各个调度的策略权重为1，因此，调度的结果为各个调度策略得分的和，然后按照得分进行排序处理。 通过如上的评判标准，k8s积分制评价出各个节点的得分值，按照得分多少，将容器运行在最佳节点上。 另：节点的调度规则是采用的plugin方式，可自行编写调度策略进行调度打分处理。 默认的优选Priorites策略如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344451. EqualPriority所有节点同样优先级，无实际效果2. ImageLocalityPriority根据主机上是否已具备Pod运行的环境来打分，得分计算：不存在所需镜像，返回0分，存在镜像，镜像越大得分越高 3. LeastRequestedPriorityCPU得分=int(((capacityMilliCPU - totalMilliCPU) * 10) / capacityMilliCPU)Memory得分= int(((capacityMemory - totalMemory) * 10) / capacityMemory)打分标准公式如下： cpu ( ( capacity - sum ( requested ) ) * 10 / capacity) + memory ( ( capacity - sum ( requested) ) * 10 / capacity ) /2例如CPU的可用资源为100，运行容器申请的资源为15，则cpu分值为8.5分，内存可用资源为100，运行容器申请资源为20，则内存分支为8分。则此评价规则在此节点的分数为(8.5 +8) / 2 = 8.25分。 4. BalanceResourceAllocation打分标准公式如下： score = 10 -abs ( cpuFraction - memoryFraction ) * 10其中， cpuFraction = requested / capacity, memoryFraction = requested / capacity 该调度策略是出于平衡度的考虑，避免出现CPU，内存消耗不均匀的事情。例如某节点的CPU剩余资源还比较充裕，假如为100，申请10，则cpuFraction为0.1，而内存剩余资源不多，假如为20，申请10，则memoryFraction为0.5，这样由于CPU和内存使用不均衡，此节点的得分为10-abs ( 0.1 - 0.5 ) * 10 = 6 分。假如CPU和内存资源比较均衡，例如两者都为0.5，那么代入公式，则得分为10分。 5. CalculateSpreadPriority此处的打分原则是： Score = 10 * （（maxCount -counts）/ （maxCount））这里主要针对多实例的情况下使用。例如，一个web服务，可能存在5个实例，例如当前节点已经分配了2个实例了，则本节点的得分为10*（（5-2）/ 5）=6分，而没有分配实例的节点，则得分为10 * （（5-0） / 5）=10分。没有分配实例的节点得分越高。6. SelectorSpreadPriority按Service和Replicaset归属计算Node上分布最少的同类Pod数量，得分计算：数量越少得分越高 7. NodePreferAvoidPodsPriority判断alpha.kubernetes.io/preferAvoidPods属性，设置权重为10000，覆盖其他策略 8. NodeAffinityPriority节点亲和性选择策略，提供两种选择器支持：requiredDuringSchedulingIgnoredDuringExecution(保证所选的主机必须满足所有Pod对主机的规则要求)preferresDuringSchedulingIgnoredDuringExecution(调度器会尽量但不保证满足NodeSelector的所有要求) 9. TaintTolerationPriority类似于Predicates策略中的PodToleratesNodeTaints，优先调度到标记了Taint的节点 10. InterPodAffinityPrioritypod亲和性选择策略，类似NodeAffinityPriority，提供两种选择器支持：requiredDuringSchedulingIgnoredDuringExecution(保证所选的主机必须满足所有Pod对主机的规则要求)preferresDuringSchedulingIgnoredDuringExecution(调度器会尽量但不保证满足NodeSelector的所有要求)，两个子策略：podAffinity 、podAntiAffinity后边会专门详解该策略 11. MostRequestedPriority动态伸缩集群环境比较适用，会优先调度pod到使用率最高的主机节点，这样在伸缩集群时，就会腾出空闲机器，从而进行停机处理。 4. 自定义调度器每个新的pod将会被默认的调度器所调用。但是如果我们提供自己的调度器，这个默认的调度器将会忽略这些pod,允许自定义的调度器来调取到节点上，如下面的例子一样.这里我们创建pod的pod-to-schedule.yaml时候执行schedulerName字段：1234567891011apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: schedulerName: my-scheduler containers: - name: nginx image: nginx:1.10 如果我们在不部署自定义调度程序的情况下创建此Pod，则默认调度程序将忽略它，并且它将保持处于挂起状态。因此，我们需要一个自定义调度程序来查找和调度其schedulerName字段为my-scheduler的pod。一个自定义的调度器可以使用任何语言来定义，可以根据你的需要来决定它的复杂度。这里一个通过bash来写的简单的调度器的例子，它随机的分配节点。注意到你需要运行kubectl proxy来配合它的工作。12345678910111213141516#!/bin/bash SERVER=&apos;localhost:8001&apos; while true; do for PODNAME in $(kubectl --server $SERVER get pods -o json | jq &apos;.items[] | select(.spec.schedulerName == &quot;my-scheduler&quot;) | select(.spec.nodeName == null) | .metadata.name&apos; | tr -d &apos;&quot;&apos;) ; do NODES=($(kubectl --server $SERVER get nodes -o json | jq &apos;.items[].metadata.name&apos; | tr -d &apos;&quot;&apos;)) NUMNODES=$&#123;#NODES[@]&#125; CHOSEN=$&#123;NODES[$[ $RANDOM % $NUMNODES ]]&#125; curl --header &quot;Content-Type:application/json&quot; --request POST --data &apos;&#123;&quot;apiVersion&quot;:&quot;v1&quot;, &quot;kind&quot;: &quot;Binding&quot;, &quot;metadata&quot;: &#123;&quot;name&quot;: &quot;&apos;$PODNAME&apos;&quot;&#125;, &quot;target&quot;: &#123;&quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot; : &quot;Node&quot;, &quot;name&quot;: &quot;&apos;$CHOSEN&apos;&quot;&#125;&#125;&apos; http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/ echo &quot;Assigned $PODNAME to $CHOSEN&quot; done sleep 1 done 步骤： In terminal A：kubectl proxy kubectl create -f pod-to-schedule.yaml 此时pod处于pending状态，等待my-scheduler bash my-scheduler.sh 1234567&#123; &quot;kind&quot;: &quot;Status&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123;&#125;, &quot;status&quot;: &quot;Success&quot;, &quot;code&quot;: 201&#125;Assigned nginx to gke-cluster-1-default-pool-ac152967-nd30 5. 参考 介绍：官网示例 配置调度算法，make生成二进制文件 当二进制文件通过Dockerfile打包进image中 定义my-scheduer.yaml，启动该scheduer_pod 部署pod.yaml中定义scheduerName为my-scheduer 6. 例子 自定义scheduler Error 12345678910111213141516171819202122[root@es1 k8s-scheduler-master]# go run main.go main.go:12:2: cannot find package &quot;github.com/topicai/candy&quot; in any of: /usr/local/go/src/github.com/topicai/candy (from $GOROOT) /home/centos/Applications/GO/src/github.com/topicai/candy (from $GOPATH)main.go:13:2: cannot find package &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot; in any of: /usr/local/go/src/k8s.io/apimachinery/pkg/apis/meta/v1 (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/apimachinery/pkg/apis/meta/v1 (from $GOPATH)main.go:14:2: cannot find package &quot;k8s.io/client-go/kubernetes&quot; in any of: /usr/local/go/src/k8s.io/client-go/kubernetes (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/kubernetes (from $GOPATH)main.go:15:2: cannot find package &quot;k8s.io/client-go/pkg/api/v1&quot; in any of: /usr/local/go/src/k8s.io/client-go/pkg/api/v1 (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/pkg/api/v1 (from $GOPATH)main.go:16:2: cannot find package &quot;k8s.io/client-go/rest&quot; in any of: /usr/local/go/src/k8s.io/client-go/rest (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/rest (from $GOPATH)main.go:17:2: cannot find package &quot;k8s.io/client-go/tools/clientcmd&quot; in any of: /usr/local/go/src/k8s.io/client-go/tools/clientcmd (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/tools/clientcmd (from $GOPATH)main.go:18:2: cannot find package &quot;k8s.io/client-go/tools/clientcmd/api&quot; in any of: /usr/local/go/src/k8s.io/client-go/tools/clientcmd/api (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/tools/clientcmd/api (from $GOPATH) 缺少包，go get -v -u github.com/topicai/candy 未进行下去 12345678[root@es1 k8s-scheduler-master]# go run main.go main.go:15:2: cannot find package &quot;k8s.io/client-go/pkg/api/v1&quot; in any of: /usr/local/go/src/k8s.io/client-go/pkg/api/v1 (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/pkg/api/v1 (from $GOPATH)[root@es1 k8s-scheduler-master]# go get -v -u k8s.io/client-go/pkg/api/v1package k8s.io/client-go/pkg/api/v1: cannot find package &quot;k8s.io/client-go/pkg/api/v1&quot; in any of: /usr/local/go/src/k8s.io/client-go/pkg/api/v1 (from $GOROOT) /home/centos/Applications/GO/src/k8s.io/client-go/pkg/api/v1 (from $GOPATH)","categories":[],"tags":[{"name":"k8s scheduler","slug":"k8s-scheduler","permalink":"http://qhzxc0015.com/tags/k8s-scheduler/"}]},{"title":"插画版Kubernetes指南","slug":"10. k8s插画","date":"2018-01-21T18:24:13.000Z","updated":"2018-01-21T18:33:50.448Z","comments":true,"path":"2018/01/22/10. k8s插画.html","link":"","permalink":"http://qhzxc0015.com/2018/01/22/10. k8s插画.html","excerpt":"原文链接是根据一个视频翻译过来的，比较形象。故事是这样的：Matt Butcher 是 Deis 的平台架构师，热爱哲学，咖啡和精雕细琢的代码。有一天女儿走进书房问他什么是 Kubernetes，于是就有了这本插画版的 Kubernetes 指南，讲述了勇敢的 Phippy（一个 PHP 应用），在 Kubernetes 的冒险故事，满满的父爱！","text":"原文链接是根据一个视频翻译过来的，比较形象。故事是这样的：Matt Butcher 是 Deis 的平台架构师，热爱哲学，咖啡和精雕细琢的代码。有一天女儿走进书房问他什么是 Kubernetes，于是就有了这本插画版的 Kubernetes 指南，讲述了勇敢的 Phippy（一个 PHP 应用），在 Kubernetes 的冒险故事，满满的父爱！ 某一天 有一天，女儿走进书房问我：『亲爱的爸爸，什么是 Kubernetes 呢？』 我回答她：『Kubernetes 是一个开源的 Docker 容器编排系统，它可以调度计算集群的节点，动态管理上面的作业，保证它们按用户期望的状态运行。通过使用「labels」和「pods」的概念，Kubernetes 将应用按逻辑单元进行分组，方便管理和服务发现。』 女儿更疑惑了……于是就有了这个故事。 给孩子的插画版 Kubernetes 指南 很久很久以前，有一个叫 Phippy 的 PHP 应用，她很单纯，只有一个页面。她住在一个托管服务里，周围还有很多可怕的应用，她都不认识，也不想去认识，但是他们却要共享这里的环境。所以，她一直都希能有一个属于自己的环境：一个可以称作 home 的 webserver。 每个应用的运行都要依赖一个环境，对于一个 PHP 应用来说，这个环境包括了一个 webserver，一个可读的文件系统和 PHP 的 engine。 有一天，一只可爱的鲸鱼拜访了 Phippy，他建议 Phippy 住在容器里。Phippy 听从了鲸鱼的建议搬家了，虽然这个容器看起来很好，但是……怎么说呢，就像是漂浮在海上的一个小房间一样，还是没有家的感觉。 不过容器倒是为应用提供了隔离的环境，在这个环境里应用就能运行起来。但是这些相互隔离的容器需要管理，也需要跟外面的世界沟通。共享的文件系统，网络，调度，负载均衡和资源分配都是挑战。 『抱歉……孩子……』鲸鱼耸耸肩，一摇尾消失在了海平面下…… Phippy 还没有来得及失望，就看到远方驶来一艘巨轮，掌舵的老船长非常威风。这艘船乍一看就是大了点，等到船走近了，Phippy 才发现船体两边挂满了皮筏。 老船长用充满智慧的语气对 Phippy 说：『你好，我是 Kube 船长』。 『Kubernetes』是希腊语中的船长，后来的『Cybernetic』和『Gubernatorial』这两个词就是从 Kubernetes 衍生来的。Kubernetes 项目由 Google 发起，旨在为生产环境中成千上万的容器，构建一个健壮的平台。 『您好，我是 Phippy。』 『很高兴认识你。』船长边说，边在 Phippy 身上放了一个 name tag。 Kubernetes 使用 label 作为『nametag』来区分事物，还可以根据 label 来查询。label 是开放式的：可以根据角色，稳定性或其它重要的特性来指定。 Kube 船长建议 Phippy 可以把她的容器搬到船上的 pod 里，Phippy 很高兴地接受了这个提议，把容器搬到了 Kube 的大船上。Phippy 感觉自己终于有家了。 在 Kubernetes 中，pod 代表着一个运行着的工作单元。通常，每个 pod 中只有一个容器，但有些情况下，如果几个容器是紧耦合的，这几个容器就会运行在同一个 pod 中。Kubernetes 承担了 pod 与外界环境通信的工作。 Phippy 对这一切都感到很新奇，同时她也有很多与众不同的关注点：『如果我想要复制自己该怎么做呢？按需的……任意次数的可以吗？』 『很简单。』船长说道，接着就给 Phippy 介绍起了 replication controller。 Replication controller 提供了一种管理任意数量 pod 的方式。一个 replication controller 包含了一个 pod 模板，这个模板可以被不限次数地复制。通过 replication controller，Kubernetes 可以管理 pod 的生命周期，包括扩/缩容，滚动部署和监控等功能。 Phippy 就这样在船上和自己的副本愉快地生活了好多天。但是每天只能面对自己的副本，这样的生活也太孤单了。 Kube 船长慷慨地笑道：『我有好东西给你。』 说着，Kube 船长就在 Phippy 的 replication controller 和船上其它地方之间建了一个隧道：『就算你们四处移动，这个隧道也会一直待在这里，它可以帮你找到其它 pod，其它 pod 也可以找到你。』 service 可以和 Kubernetes 环境中其它部分（包括其它 pod 和 replication controller）进行通信，告诉它们你的应用提供什么服务。Pod 可以四处移动，但是 service 的 IP 地址和端口号是不变的。而且其它应用可以通过 Kubernetes 的服务发现找到你的 service。 有了 service，Phippy 终于敢去船上其它地方去玩了，她很快就有了新朋友 Goldie。有一天，Goldie 送了 Phippy 一件礼物，没想到 Phippy 只看了一眼就哭了。 『你怎么哭了？』Goldie 问道。 『我太喜欢这个礼物了，可惜没地儿放……』Phippy 都开始抽泣了。Goldie 一听原来是这么回事，马上就告诉 Phippy：『为什么不放在一个 volume 里呢？』 Volume 代表了一块容器可以访问和存储信息的空间，对于应用来说，volume 是一个本地的文件系统。实际上，除了本地存储，Ceph、Gluster、Elastic Block Storage 和很多其它后端存储都可以作为 volume。 Phippy 渐渐地爱上了船上的生活，她很享受和新朋友的相处（Goldie 的每个 pod 副本也都很 nice）。但是回想起以前的生活，她又在想是不是可以有一点点私人空间呢？ Kube 船长很理解：『看起来你需要 namespace。』 Namespace 是 Kubernetes 内的分组机制。Service，pod，replication controller 和 volume 可以很容易地和 namespace 配合工作，但是 namespace 为集群中的组件间提供了一定程度的隔离。 于是，在 Kube 船长的船上，Phippy 和她的朋友们开始了海上的历险，最重要的是，Phippy 找到了自己的家。从此，Phippy 过上了幸福的生活。","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://qhzxc0015.com/tags/k8s/"}]},{"title":"kubeadm 1.8.1","slug":"09. kubeadm-1.8.1","date":"2018-01-07T15:57:59.000Z","updated":"2018-01-09T01:03:20.878Z","comments":true,"path":"2018/01/07/09. kubeadm-1.8.1.html","link":"","permalink":"http://qhzxc0015.com/2018/01/07/09. kubeadm-1.8.1.html","excerpt":"Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice “fast paths” for creating Kubernetes clusters.","text":"Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice “fast paths” for creating Kubernetes clusters. 1.准备工作 关闭防火墙查看状态：firewall-cmd --state临时关闭：sudo systemctl stop firewall关自启动：sudo systemctl disable firewalld.service 关闭swap查看状态：free临时关闭：sudo swapoff -a永久关闭：在/etc/sysctl.conf文件里添加如下参数：vm.swappiness=10 关闭SELinux查看SELinux状态：/usr/sbin/sestatus -v临时关闭（不用重启机器）：setenforce 02、修改配置文件需要重启机器：修改/etc/selinux/config 文件将SELINUX=enforcing改为SELINUX=disabled 时间同步ntpsudo yum install -y ntpdatesudo ntpdate -u ntp.api.bzsudo systemctl enable ntpd.servicesudo systemctl status ntpd.service其中/etc/ntp.conf可以添加NTP服务器，比如：161.3.16.172.in-addr.arpa 安装dockersudo yum install -y dockersudo systemctl enable docker &amp;&amp; sudo systemctl start docker如果不做这四个准备工作会遇到各种奇怪的问题： 防火墙不关，pod会no route to host swap不关，重启kube服务会不加载 SELinux不关，kubeadm init失败 ntp时间不同步，node加载后notready 2. 安装 master节点执行root用户sh init-master.sh 由于安全原因，默认情况下pod不会被schedule到master节点上，可以通过下面命令解除这种限制：kubectl taint nodes --all node-role.kubernetes.io/master- 在master安装dashboard，这样dashboard不会调用到node上执行sh init-dashboard.sh把type:ClusterIP改成NodePort然后访问dashboard所在节点+端口号即可。 node节点执行init-node.sh再执行master输出的kubeadm join命令 3 常见问题 重新安装kubeadm resetkubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version v1.8.1初始化完成后需要执行命令： 123456非root：mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configroot:export KUBECONFIG=/etc/kubernetes/admin.conf dial tcp [::1]:10255: getsockopt: connection refused. 123456789101112131415[kubelet-check] It seems like the kubelet isn't running or healthy.[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.Unfortunately, an error has occurred: timed out waiting for the conditionThis error is likely caused by that: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) - There is no internet connection; so the kubelet can't pull the following control plane images: - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1 - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1 - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1You can troubleshoot this for example with the following commands if you're on a systemd-powered system: - 'systemctl status kubelet' - 'journalctl -xeu kubelet'couldn't initialize a Kubernetes cluster 通过journalctl -xeu kubelet1234Jan 08 08:53:13 kube1 kubelet[18485]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &quot;systemd&quot; is different from docker cgroup driver: &quot;cgroupfs&quot;Jan 08 08:53:13 kube1 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILUREJan 08 08:53:13 kube1 systemd[1]: Unit kubelet.service entered failed state.Jan 08 08:53:13 kube1 systemd[1]: kubelet.service failed. 修改cgroup driver有两种方式：1.修改kubelet service即vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf把KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs #这个配置与docker改成一致2.修改docker service即vim /lib/systemd/system/docker.service把--exec-opt native.cgroupdriver=cgroupfs修改systemd The connection to the server localhost:8080 was refused - did you specify the right host or port? calico启动1/2 kube-dns不启动 etcd没有删除清楚rm /var/lib/etcd","categories":[],"tags":[{"name":"kubernetes kubeadm","slug":"kubernetes-kubeadm","permalink":"http://qhzxc0015.com/tags/kubernetes-kubeadm/"}]},{"title":"kubernetes for Elasticsearch","slug":"08.k8s for es","date":"2017-12-01T22:56:03.000Z","updated":"2017-12-14T17:28:44.917Z","comments":true,"path":"2017/12/02/08.k8s for es.html","link":"","permalink":"http://qhzxc0015.com/2017/12/02/08.k8s for es.html","excerpt":"ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，其部署简单扩展方便，所以用k8s+es实践。","text":"ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，其部署简单扩展方便，所以用k8s+es实践。 参考文献： 使用k8s编排ElasticSearch集群 Elasticsearch for Kubernetes CentOS7.2使用yum安装kubernetes ETCD系列 1. Kubernetes搭建0.0 安装完毕启动服务1234567master:sudo systemctl start kube-apiserver.service kube-controller-manager.service kube-scheduler.servicesudo systemctl stop kube-apiserver.service kube-controller-manager.service kube-scheduler.servicenode:sudo systemctl start docker flanneld kube-proxy kubeletsudo systemctl stop docker flanneld kube-proxy kubelet 1.1 准备工作1.1.1 关闭防火墙1234systemctl stop firewalld systemctl disable firewalld 查看状态：firewall-cmd --state 1.1.2 安装NTP123yum -y install ntp systemctl start ntpd systemctl enable ntpd 1.1.3 禁用selinux执行命令：vim /etc/selinux/config1234#SELINUX=enforcing SELINUX=disabled 查看状态：/usr/sbin/sestatus -v 2.2 搭建kubernetes2.1.1 环境配置以及角色 IP Hosts Role 202.193.74.179 master,etcd etcd kube-apiserver kube-scheduler kube-controller-manage flannel 202.193.75.80 node1 docker kube-proxy kubelet flannel 202.193.75.34 node2 docker kube-proxy kubelet flannel 2.1.2 安装etcd etcd用于服务发现和服务注册，解决客户端如何知道多节点上服务的IP地址和端口问题。Etcd组件作为一个高可用强一致性的服务发现存储仓库,etcd服务需要提供kubernetes集群的所有节点，因此需要监听于可用于外部通信的地址。 安装方法一单etcd节点，步骤如下:执行命令：yum install -y etcd检测安装：rpm -ql etcd 1234567891011121314[centos@179 es]$ rpm -ql etcd/etc/etcd/etc/etcd/etcd.conf/usr/bin/etcd/usr/bin/etcdctl/usr/lib/systemd/system/etcd.service/usr/share/doc/etcd-3.1.9/usr/share/doc/etcd-3.1.9/CONTRIBUTING.md/usr/share/doc/etcd-3.1.9/README.md/usr/share/doc/etcd-3.1.9/ROADMAP.md/usr/share/doc/etcd-3.1.9/glide.lock/usr/share/licenses/etcd-3.1.9/usr/share/licenses/etcd-3.1.9/LICENSE/var/lib/etcd ​ 对其中/etc/etcd/etcd.conf文件进行配置sudo vim /etc/etcd/etcd.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344# [member]ETCD_NAME=defaultETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ETCD_WAL_DIR=&quot;&quot;#ETCD_SNAPSHOT_COUNT=&quot;10000&quot;#ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;#ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;http://master:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://master:2379&quot;#ETCD_MAX_SNAPSHOTS=&quot;5&quot;#ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;##[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://master:2380&quot;# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;default=http://etcd:2380&quot;#ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://master:2379&quot;#ETCD_DISCOVERY=&quot;&quot;#ETCD_DISCOVERY_SRV=&quot;&quot;#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;#ETCD_DISCOVERY_PROXY=&quot;&quot;#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;##[proxy]#ETCD_PROXY=&quot;off&quot;#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;##[security]#ETCD_CERT_FILE=&quot;&quot;#ETCD_KEY_FILE=&quot;&quot;#ETCD_CLIENT_CERT_AUTH=&quot;false&quot;#ETCD_TRUSTED_CA_FILE=&quot;&quot;#ETCD_AUTO_TLS=&quot;false&quot;#ETCD_PEER_CERT_FILE=&quot;&quot;#ETCD_PEER_KEY_FILE=&quot;&quot;#ETCD_PEER_CLIENT_CERT_AUTH=&quot;false&quot; 配置完毕，重启服务执行命令：sudo systemctl restart etcd查看集群健康程度：etcdctl -C http://master:2379 cluster-health 12member 8e9e05c52164694d is healthy: got healthy result from http://master:2379cluster is healthy etcd常用操作创建目录：etcdctl -C http://master:2379 mkdir /testdir查看目录：etcdctl -C http://master:2379 ls给newkey一个helloetcd值：etcdctl -C http://master:2379 mk /testdir/newkey helloetcd 安装方法二集群安装，默认安装方式，通过配置信息来启动集群sudo yum install etcd 123456789101112131415161718192021222324252627nohup etcd --name infra0 --initial-advertise-peer-urls http://202.193.75.18:2380 \\--listen-peer-urls http://202.193.75.18:2380 \\--listen-client-urls http://202.193.75.18:2379,http://127.0.0.1:2379 \\--advertise-client-urls http://202.193.75.18:2379 \\--initial-cluster-token etcd-cluster-1 --initial-cluster infra0=http://202.193.75.18:2380,infra1=http://202.193.75.142:2380,infra2=http://202.193.74.159:2380,infra3=http://202.193.75.125:2380 \\--initial-cluster-state new &amp;nohup etcd --name infra1 --initial-advertise-peer-urls http://202.193.75.142:2380 \\--listen-peer-urls http://202.193.75.142:2380 \\--listen-client-urls http://202.193.75.142:2379,http://127.0.0.1:2379 \\--advertise-client-urls http://202.193.75.142:2379 \\--initial-cluster-token etcd-cluster-1 --initial-cluster infra0=http://202.193.75.18:2380,infra1=http://202.193.75.142:2380,infra2=http://202.193.74.159:2380,infra3=http://202.193.75.125:2380 \\--initial-cluster-state new &amp;nohup etcd --name infra2 --initial-advertise-peer-urls http://202.193.74.159:2380 \\--listen-peer-urls http://202.193.74.159:2380 \\--listen-client-urls http://202.193.74.159:2379,http://127.0.0.1:2379 \\--advertise-client-urls http://202.193.74.159:2379 \\--initial-cluster-token etcd-cluster-1 --initial-cluster infra0=http://202.193.75.18:2380,infra1=http://202.193.75.142:2380,infra2=http://202.193.74.159:2380,infra3=http://202.193.75.125:2380 \\--initial-cluster-state new &amp; nohup etcd --name infra3 --initial-advertise-peer-urls http://202.193.75.125:2380 \\--listen-peer-urls http://202.193.75.125:2380 \\--listen-client-urls http://202.193.75.125:2379,http://127.0.0.1:2379 \\--advertise-client-urls http://202.193.75.125:2379 \\--initial-cluster-token etcd-cluster-1 --initial-cluster infra0=http://202.193.75.18:2380,infra1=http://202.193.75.142:2380,infra2=http://202.193.74.159:2380,infra3=http://202.193.75.125:2380 \\--initial-cluster-state new &amp; ​ 集群成员：etcdctl member list123454535d978ca77659: name=infra0 peerURLs=http://202.193.75.18:2380 clientURLs=http://202.193.75.18:2379 isLeader=true5d3b6e02afb313fe: name=infra1 peerURLs=http://202.193.75.142:2380 clientURLs=http://202.193.75.142:2379 isLeader=false63e7ce5e5aef542b: name=infra3 peerURLs=http://202.193.75.125:2380 clientURLs=http://202.193.75.125:2379 isLeader=false7356f43ba0335cf9: name=infra2 peerURLs=http://202.193.74.159:2380 clientURLs=http://202.193.74.159:2379 isLeader=false ​ 集群健康：etcdctl cluster-health12345member 54535d978ca77659 is healthy: got healthy result from http://202.193.75.18:2379member 5d3b6e02afb313fe is healthy: got healthy result from http://202.193.75.142:2379member 63e7ce5e5aef542b is healthy: got healthy result from http://202.193.75.125:2379member 7356f43ba0335cf9 is healthy: got healthy result from http://202.193.74.159:2379cluster is healthy 2.1.3 安装kubernetes master节点查看所有kubernetes安装包:yum list all kubernetes* 12345678910已加载插件：fastestmirror, langpacksDetermining fastest mirrors * base: centos.ustc.edu.cn * extras: mirrors.sohu.com * updates: mirrors.aliyun.comkubernetes-client.x86_64 1.5.2-0.7.git269f928.el7 @extraskubernetes-node.x86_64 1.5.2-0.7.git269f928.el7 @extraskubernetes.x86_64 1.5.2-0.7.git269f928.el7 extras kubernetes-master.x86_64 1.5.2-0.7.git269f928.el7 extras kubernetes-unit-test.x86_64 1.5.2-0.7.git269f928.el7 extras 12345678910已加载插件：fastestmirror, langpacksLoading mirror speeds from cached hostfile * base: mirrors.nwsuaf.edu.cn * extras: ftp.sjtu.edu.cn * updates: mirrors.aliyun.comkubernetes-client.x86_64 1.5.2-0.7.git269f928.el7 kubernetes-master.x86_64 1.5.2-0.7.git269f928.el7 kubernetes.x86_64 1.5.2-0.7.git269f928.el7 kubernetes-node.x86_64 1.5.2-0.7.git269f928.el7 kubernetes-unit-test.x86_64 1.5.2-0.7.git269f928.el7 安装master：sudo yum install -y kubernetes-master安装kubernetes-master的时候也安装了kubernetes-client。查看安全：rpm -ql kubernetes-master123456789101112131415161718192021222324/etc/kubernetes/etc/kubernetes/apiserver/etc/kubernetes/config/etc/kubernetes/controller-manager/etc/kubernetes/scheduler/run/kubernetes/usr/bin/hyperkube/usr/bin/kube-apiserver/usr/bin/kube-controller-manager/usr/bin/kube-scheduler/usr/lib/systemd/system/kube-apiserver.service/usr/lib/systemd/system/kube-controller-manager.service/usr/lib/systemd/system/kube-scheduler.service/usr/lib/tmpfiles.d/kubernetes.conf/usr/share/doc/kubernetes-master-1.5.2/usr/share/doc/kubernetes-master-1.5.2/CHANGELOG.md/usr/share/doc/kubernetes-master-1.5.2/CONTRIBUTING.md/usr/share/doc/kubernetes-master-1.5.2/README.md/usr/share/doc/kubernetes-master-1.5.2/code-of-conduct.md/usr/share/licenses/kubernetes-master-1.5.2/usr/share/licenses/kubernetes-master-1.5.2/LICENSE/usr/share/man/man1/kube-apiserver.1.gz/usr/share/man/man1/kube-controller-manager.1.gz/usr/share/man/man1/kube-scheduler.1.gz 查看安装：rpm -ql kubernetes-client1234567891011/usr/bin/hyperkube/usr/bin/kubectl/usr/share/bash-completion/completions/kubectl/usr/share/doc/kubernetes-client-1.5.2/usr/share/doc/kubernetes-client-1.5.2/CHANGELOG.md/usr/share/doc/kubernetes-client-1.5.2/CONTRIBUTING.md/usr/share/doc/kubernetes-client-1.5.2/README.md/usr/share/doc/kubernetes-client-1.5.2/code-of-conduct.md/usr/share/licenses/kubernetes-client-1.5.2/usr/share/licenses/kubernetes-client-1.5.2/LICENSE..... 对其中的apiserver进行修改sudo vim /etc/kubernetes/apiserver1234567891011121314151617181920#### kubernetes system config## The following values are used to configure the kube-apiserver## The address on the local server to listen to.KUBE_API_ADDRESS=&quot;--insecure-bind-address=0.0.0.0&quot;# The port on the local server to listen on.# KUBE_API_PORT=&quot;--port=8080&quot;# Port minions listen on# KUBELET_PORT=&quot;--kubelet_port=10250&quot;# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=http://etcd:2379&quot;# Address range to use for servicesKUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;# default admission control policies# KUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;KUBE_ADMISSION_CONTROL=&quot;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot;# Add your own!KUBE_API_ARGS=&quot;&quot; 注意：KUBE_ADMISSION_CONTROL选项里删除ServiceAccount选项。不用帐号密码认证，麻烦。 ​ 对config进行修改sudo vim /etc/kubernetes/config 12345678910111213141516171819#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://master:8080&quot; ​ 配置完成，启动服务kubernetes-mastersudo systemctl start kube-apiserver.service kube-controller-manager.service kube-scheduler.service node节点 docker安装：yum install -y docker kubernetes-node安装：yum install -y kubernetes-node查看安装rpm -ql kubernetes-node 12345678910111213141516171819202122/etc/kubernetes/etc/kubernetes/config/etc/kubernetes/kubelet/etc/kubernetes/proxy/etc/systemd/system.conf.d/kubernetes-accounting.conf/run/kubernetes/usr/bin/hyperkube/usr/bin/kube-proxy/usr/bin/kubelet/usr/lib/systemd/system/kube-proxy.service/usr/lib/systemd/system/kubelet.service/usr/lib/tmpfiles.d/kubernetes.conf/usr/share/doc/kubernetes-node-1.5.2/usr/share/doc/kubernetes-node-1.5.2/CHANGELOG.md/usr/share/doc/kubernetes-node-1.5.2/CONTRIBUTING.md/usr/share/doc/kubernetes-node-1.5.2/README.md/usr/share/doc/kubernetes-node-1.5.2/code-of-conduct.md/usr/share/licenses/kubernetes-node-1.5.2/usr/share/licenses/kubernetes-node-1.5.2/LICENSE/usr/share/man/man1/kube-proxy.1.gz/usr/share/man/man1/kubelet.1.gz/var/lib/kubelet 修改sudo vim /etc/kubernetes/kubelet1234567891011121314151617181920#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)KUBELET_ADDRESS=&quot;--address=0.0.0.0&quot;# The port for the info server to serve on# KUBELET_PORT=&quot;--port=10250&quot;# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=&quot;--hostname-override=node1&quot;# location of the api-serverKUBELET_API_SERVER=&quot;--api-servers=http://master:8080&quot;# pod infrastructure containerKUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&quot;# Add your own!KUBELET_ARGS=&quot;--pod-infra-container-image=kubernetes/pause&quot; 修改sudo vim /etc/kubernetes/config1234567891011121314151617181920212223#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot;KUBE_MASTER=&quot;--master=http://master:8080&quot; 启动服务：sudo systemctl start kubelet.service kube-proxy.service添加节点，config不变，只需修改kubelet启动服务后可以查看端口netstat -tln12345678910111213141516Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:10249 0.0.0.0:* LISTEN tcp 0 0 192.168.122.1:53 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN tcp6 0 0 :::10250 :::* LISTEN tcp6 0 0 :::10255 :::* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp6 0 0 ::1:631 :::* LISTEN tcp6 0 0 :::31480 :::* LISTEN tcp6 0 0 ::1:25 :::* LISTEN tcp6 0 0 :::31451 :::* LISTEN tcp6 0 0 :::4194 :::* LISTEN tcp6 0 0 :::31685 :::* LISTEN 其中4194端口为cAdvisor服务，可以查看资源消耗 2.1.3 安装flannel k8s的网络管理用flannel网络。所以三个节点(master,node1,node2)都要装。 执行命令：sudo yum install -y flannel查看安装：rpm -ql flannel123456789101112131415/run/flannel/usr/bin/flanneld/usr/bin/flanneld-start/usr/lib/systemd/system/docker.service.d/flannel.conf/usr/lib/systemd/system/flanneld.service/usr/lib/tmpfiles.d/flannel.conf/usr/libexec/flannel/usr/libexec/flannel/mk-docker-opts.sh/usr/share/doc/flannel-0.7.1/usr/share/doc/flannel-0.7.1/CONTRIBUTING.md/usr/share/doc/flannel-0.7.1/DCO/usr/share/doc/flannel-0.7.1/LICENSE/usr/share/doc/flannel-0.7.1/MAINTAINERS/usr/share/doc/flannel-0.7.1/NOTICE/usr/share/doc/flannel-0.7.1/README.md sudo vim /etc/sysconfig/flanneld1234567891011# Flanneld configuration options # etcd url location. Point this to the server where etcd runsFLANNEL_ETCD_ENDPOINTS=&quot;http://etcd:2379&quot;# etcd config key. This is the configuration key that flannel queries# For address range assignmentFLANNEL_ETCD_PREFIX=&quot;/coreos.com/network&quot;# Any additional options that you want to passFANNEL_OPTIONS=&quot; -iface=ens33&quot; master需要etcd讲键值存入(只需要master进行修改即可)：etcdctl -C http://etcd1:2379 mk /coreos.com/network/config &#39;{&quot;Network&quot;:&quot;10.7.0.0/16&quot;}&#39;启动flannel服务：sudo systemctl start flanneld.service通过ifconfig进行查看，这时docker0与flannel0处于同一网段12345678910111213141516171819202122232425docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1472 inet 10.7.8.1 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::42:97ff:fe39:3861 prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:97:39:38:61 txqueuelen 0 (Ethernet) RX packets 53 bytes 3474 (3.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 46 bytes 5360 (5.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 202.193.75.34 netmask 255.255.254.0 broadcast 202.193.75.255 inet6 fe80::20c:29ff:fe65:c10f prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:65:c1:0f txqueuelen 1000 (Ethernet) RX packets 34213307 bytes 4678823447 (4.3 GiB) RX errors 0 dropped 2 overruns 0 frame 0 TX packets 1419255 bytes 781178180 (744.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt; mtu 1472 inet 10.7.8.0 netmask 255.255.0.0 destination 10.7.8.0 unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 txqueuelen 500 (UNSPEC) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 至此，k8s搭建完成，查看集群状态kubectl get node1234NAME STATUS AGEnew2 Ready 13mnew3 Ready 13mnew4 Ready 13m 3.1 搭建es3.1.1 拉取镜像执行命令：sudo docker docker.io/kayrus/docker-elasticsearch-kubernetes:1.7.1 部署文件es-rc.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: ReplicationControllermetadata: name: es labels: component: elasticsearchspec: replicas: 1 template: metadata: labels: component: elasticsearch spec: serviceAccount: elasticsearch containers: - name: es securityContext: capabilities: add: - IPC_LOCK image: docker.io/kayrus/docker-elasticsearch-kubernetes:1.7.1 env: - name: KUBERNETES_CA_CERTIFICATE_FILE value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: &quot;CLUSTER_NAME&quot; value: &quot;myesdb&quot; - name: &quot;DISCOVERY_SERVICE&quot; value: &quot;elasticsearch&quot; - name: NODE_MASTER value: &quot;true&quot; - name: NODE_DATA value: &quot;true&quot; - name: HTTP_ENABLE value: &quot;true&quot; ports: - containerPort: 9200 name: http protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - mountPath: /data name: storage volumes: - name: storage emptyDir: &#123;&#125; es-svc.yaml1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: elasticsearch labels: component: elasticsearchspec: type: LoadBalancer selector: component: elasticsearch ports: - name: http port: 9200 protocol: TCP - name: transport port: 9300 protocol: TCP service-account.yaml1234apiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch 部署指令：kubectl create -f .查看工作：kubectl get pods123[centos@179 EFK]$ kubectl get podsNAME READY STATUS RESTARTS AGEes-3c69r 1/1 Running 0 1d 查看日志：kubectl logs es-3c69r123456789101112[centos@179 EFK]$ kubectl logs es-3c69r[2017-12-01 22:50:17,664][INFO ][node ] [Legion] version[1.7.1], pid[1], build[b88f43f/2015-07-29T09:54:16Z][2017-12-01 22:50:17,664][INFO ][node ] [Legion] initializing ...[2017-12-01 22:50:17,811][INFO ][plugins ] [Legion] loaded [], sites [][2017-12-01 22:50:17,877][INFO ][env ] [Legion] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [9.7gb], net total_space [9.9gb], types [rootfs][2017-12-01 22:50:20,781][INFO ][node ] [Legion] initialized[2017-12-01 22:50:20,783][INFO ][node ] [Legion] starting ...[2017-12-01 22:50:21,052][INFO ][transport ] [Legion] bound_address &#123;inet[/0:0:0:0:0:0:0:0:9300]&#125;, publish_address &#123;inet[/10.7.28.4:9300]&#125;[2017-12-01 22:50:21,088][INFO ][discovery ] [Legion] elasticsearch/tfcW-qCYTa6Eq27G-ts6-Q[2017-12-01 22:50:24,172][INFO ][cluster.service ] [Legion] detected_master [Lockdown][TepMZvL3T_O5Yu2YeMPn_A][es-qg67z][inet[/10.7.28.2:9300]], added &#123;[Lockdown][TepMZvL3T_O5Yu2YeMPn_A][es-qg67z][inet[/10.7.28.2:9300]],&#125;, reason: zen-disco-receive(from master [[Lockdown][TepMZvL3T_O5Yu2YeMPn_A][es-qg67z][inet[/10.7.28.2:9300]]])[2017-12-01 22:50:24,223][INFO ][http ] [Legion] bound_address &#123;inet[/0:0:0:0:0:0:0:0:9200]&#125;, publish_address &#123;inet[/10.7.28.4:9200]&#125;[2017-12-01 22:50:24,223][INFO ][node ] [Legion] started 扩展节点Scale命令：kubectl scale --replicas=3 rc es12345[centos@179 EFK]$ kubectl get podsNAME READY STATUS RESTARTS AGEes-3c69r 1/1 Running 0 1des-qg67z 1/1 Running 0 1des-z11hm 1/1 Running 0 1d 查看日志1234567891011121314[centos@179 EFK]$ kubectl logs es-qg67z[2017-12-01 22:49:45,616][INFO ][node ] [Lockdown] version[1.7.1], pid[1], build[b88f43f/2015-07-29T09:54:16Z][2017-12-01 22:49:45,617][INFO ][node ] [Lockdown] initializing ...[2017-12-01 22:49:45,738][INFO ][plugins ] [Lockdown] loaded [], sites [][2017-12-01 22:49:45,799][INFO ][env ] [Lockdown] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [9.7gb], net total_space [9.9gb], types [rootfs][2017-12-01 22:49:48,226][INFO ][node ] [Lockdown] initialized[2017-12-01 22:49:48,227][INFO ][node ] [Lockdown] starting ...[2017-12-01 22:49:48,397][INFO ][transport ] [Lockdown] bound_address &#123;inet[/0:0:0:0:0:0:0:0:9300]&#125;, publish_address &#123;inet[/10.7.28.2:9300]&#125;[2017-12-01 22:49:48,441][INFO ][discovery ] [Lockdown] elasticsearch/TepMZvL3T_O5Yu2YeMPn_A[2017-12-01 22:49:52,221][INFO ][cluster.service ] [Lockdown] new_master [Lockdown][TepMZvL3T_O5Yu2YeMPn_A][es-qg67z][inet[/10.7.28.2:9300]], reason: zen-disco-join (elected_as_master)[2017-12-01 22:49:52,272][INFO ][http ] [Lockdown] bound_address &#123;inet[/0:0:0:0:0:0:0:0:9200]&#125;, publish_address &#123;inet[/10.7.28.2:9200]&#125;[2017-12-01 22:49:52,273][INFO ][node ] [Lockdown] started[2017-12-01 22:49:52,291][INFO ][gateway ] [Lockdown] recovered [0] indices into cluster_state[2017-12-01 22:50:24,144][INFO ][cluster.service ] [Lockdown] added &#123;[Legion][tfcW-qCYTa6Eq27G-ts6-Q][es-3c69r][inet[/10.7.28.4:9300]],&#125;, reason: zen-disco-receive(join from node[[Legion][tfcW-qCYTa6Eq27G-ts6-Q][es-3c69r][inet[/10.7.28.4:9300]]]) 查看服务：kubectl get svc123[centos@179 EFK]$ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearch 10.254.156.16 &lt;pending&gt; 9200:31451/TCP,9300:31685/TCP 1d 通过访问31451可以服务12345678910111213141516[centos@179 EFK]$ curl node2:31451/_cluster/health?pretty&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;status&quot; : &quot;green&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 2, &quot;number_of_data_nodes&quot; : 2, &quot;active_primary_shards&quot; : 0, &quot;active_shards&quot; : 0, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 0, &quot;delayed_unassigned_shards&quot; : 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot; : 0&#125;","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://qhzxc0015.com/tags/kubernetes/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://qhzxc0015.com/tags/Elasticsearch/"}]},{"title":"CoreOS","slug":"05. coreos搭建","date":"2017-08-17T08:32:31.000Z","updated":"2017-09-09T06:52:04.136Z","comments":true,"path":"2017/08/17/05. coreos搭建.html","link":"","permalink":"http://qhzxc0015.com/2017/08/17/05. coreos搭建.html","excerpt":"CoreOS是一个基于Linux 内核的轻量级操作系统，为了计算机集群的基础设施建设而生，专注于自动化，轻松部署，安全，可靠，规模化。作为一个操作系统，CoreOS 提供了在应用容器内部署应用所需要的基础功能环境以及一系列用于服务发现和配置共享的内建工具。","text":"CoreOS是一个基于Linux 内核的轻量级操作系统，为了计算机集群的基础设施建设而生，专注于自动化，轻松部署，安全，可靠，规模化。作为一个操作系统，CoreOS 提供了在应用容器内部署应用所需要的基础功能环境以及一系列用于服务发现和配置共享的内建工具。 参考文献： 使用Vagrant在Windows下部署开发环境 准备工作： 下载安装VirtualBox 下载安装Vagrant git获取Vagrantfile：git clone https://github.com/coreos/coreos-vagrant.git 下载完毕后进入coreos-vagrant文件夹，将config.rb.sample和user-data.sample的sample去掉，修改config.rb中 12$num_instances=1 $update_channel=&apos;alpha&apos; 操作： 启动：vagrant up 状态：vagrant status 进入：vagrant ssh core-01","categories":[],"tags":[{"name":"CoreOS","slug":"CoreOS","permalink":"http://qhzxc0015.com/tags/CoreOS/"},{"name":"Docker","slug":"Docker","permalink":"http://qhzxc0015.com/tags/Docker/"}]},{"title":"Docker Registry","slug":"06. Registry搭建","date":"2017-08-17T08:22:58.000Z","updated":"2017-12-14T17:25:20.378Z","comments":true,"path":"2017/08/17/06. Registry搭建.html","link":"","permalink":"http://qhzxc0015.com/2017/08/17/06. Registry搭建.html","excerpt":"GFW真的强，所以国外镜像真的太难拉下来了,因此采用 docker registry 来存储镜像供其他节点使用","text":"GFW真的强，所以国外镜像真的太难拉下来了,因此采用 docker registry 来存储镜像供其他节点使用 1. Docker版 国外镜像真的太难拉下来了,所以采用 docker registry 来存储镜像供其他节点使用, 拉取registry镜像sudo docker pull hub.c.163.com/library/registry:2.6.0 启动仓库sudo docker run -d -p 5000:5000 -v /docker/registry:/tmp/registry hub.c.163.com/library/registry:2.6.0 将各个镜像上传到仓库 打标签：sudo docker tag gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.2 202.193.74.222/k8s-dns-dnsmasq-nanny-amd64:1.14.2上传到仓库：sudo docker push 202.193.74.222:5000/k8s-dns-dnsmasq-nanny-amd64:1.14.2 2. Harbor版参考文献： harbor搭建 harbor搭建与问题修改 Harbor也是基于registry镜像进行搭建的,可视化界面比较友好,官网下载分为在线版和离线版,建议用离线版,因为在线版需要自己在国外网站拉取镜像,伟大的墙的威力还是有的,离线版就简单了,直接有版本对应好的镜像. 1.环境与配置 安装环境:docker1.10+,python2.7+,docker-compose1.6.0+下载离线安装包harbor-offline,配置harbor.cfg修改: 123hostname = n2db_password = adminharbor_admin_password = admin #这个是登录密码 2.启动与访问 通过docker-compose.yml修改端口号再通过./prepare来更新配置之后进行./install.sh来安装最后通过docker-compose up -d来启动启动后通过n2:80来访问仓库 3.登录与上传 先进行登录docker login n2:80提示Login Succeeded为安装成功注:这里上传需要先在web端建好项目名,比如上传到仓库名library中: 12sudo docker tag n2:80/library/test:v1sudo docker push n2:80/library/test:v1 3. 阿里云版使用步骤： 在阿里云镜像页面的管理中心即可进入镜像中心 到容器镜像服务点击自己上传镜像的地区，比如华南1，创建镜像仓库 注意镜像仓库为某一镜像的不同版本控制仓库，即此仓库需要命名为镜像名称，比如仓库名etcd表示registry.cn-shenzhen.aliyuncs.com/qhzxc0015/etcd:3.0.17的镜像 在本地登录阿里云仓库sudo docker login --username=qhzxc0010@163.com registry.cn-shenzhen.aliyuncs.com 本地镜像打标签sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/qhzxc0015/etcd:3.0.17 上传本地镜像到阿里云仓库sudo docker push registry.cn-shenzhen.aliyuncs.com/qhzxc0015/etcd:3.0.17 镜像拉取sudo docker pull registry.cn-shenzhen.aliyuncs.com/qhzxc0015/etcd:3.0.17 4. ErrorsE1: 1234test@Test1:~$ sudo docker push 202.193.74.222:5000/test:latest# ERRORThe push refers to a repository [202.193.74.222:5000/test]Get https://202.193.74.222:5000/v1/_ping: http: server gave HTTP response to HTTPS client 解决方法1:在/etc/docker/目录下，创建daemon.json文件。在文件中写入： 1&#123; &quot;insecure-registries&quot;:[&quot;ip:5000&quot;] &#125; 解决方法2:Centos中vim /etc/sysconfig/docker,在文件中添加: 12ADD_REGISTRY=&apos;--add-registry e2:80&apos;INSECURE_REGISTRY=&apos;--insecure-registry e2:80&apos; 解决方法3:Ubuntu中vim /etc/default/docker,在文件中添加: 1DOCKER_OPTS=&quot;--dns 8.8.8.8 --dns 8.8.4.4 --insecure-registry=n2:80&quot; 重启dockersudo systemctl restart docker E2: 12345test@Test1:~$ sudo docker push n2:80/library/test:v185782553e37a: Waiting 745f5be9952c: Waiting denied: requested access to the resource is denied 解决方法1: 先登录才可以上传,进行登录操作 解决方法2: 用hosts名称会异常,建议直接使用ip E3: 1ERROR: for proxy Cannot start service proxy: driver failed programming external connectivity on endpoint nginx (fdeb3e538d5f8d714ea5c79a9f3f127f05f7ba5d519e09c4c30ef81f40b2fe77): Error starting userland proxy: listen tcp 0.0.0.0:80: bind: address already in use 端口占用问题,可以通过修改端口来完成,在harbor/docker-compose.yml中修改,比如此处的80端口修改为8080操作如下: 12345678910111213141516171819202122proxy: image: vmware/nginx:1.11.5-patched container_name: nginx restart: always volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor ports: - 8080:80 - 444:443 - 4443:4443 depends_on: - mysql - registry - ui - log logging: driver: &quot;syslog&quot; options: syslog-address: &quot;tcp://127.0.0.1:1514&quot; tag: &quot;proxy&quot; 注意这里修改了之后在上面问题E1,E2中的修改也端口 5. Shells批量上传脚本push_images.sh1234567#!/bin/bashimages=(kube-proxy-amd64:v1.6.6 kube-scheduler-amd64:v1.6.6 kube-controller-manager-amd64:v1.6.6 kube-apiserver-amd64:v1.6.6 etcd-amd64:3.0.17 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.6.1 k8s-dns-sidecar-amd64:1.14.2 k8s-dns-kube-dns-amd64:1.14.2 k8s-dns-dnsmasq-nanny-amd64:1.14.2 etcd:v3.1.5)for imageName in $&#123;images[@]&#125; ; do sudo docker tag gcr.io/google_containers/$imageName e2:5000/$imageName sudo docker push e2:5000/$imageName sudo docker rmi e2:5000/$imageNamedone 批量下载脚本pull_images.sh1234567#!/bin/bash## 标题 ##images=(kube-proxy-amd64:v1.6.6 kube-scheduler-amd64:v1.6.6 kube-controller-manager-amd64:v1.6.6 kube-apiserver-amd64:v1.6.6 etcd-amd64:3.0.17 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.6.1 k8s-dns-sidecar-amd64:1.14.2 k8s-dns-kube-dns-amd64:1.14.2 k8s-dns-dnsmasq-nanny-amd64:1.14.2 etcd:v3.1.5)for imageName in $&#123;images[@]&#125; ; do sudo docker pull e2:5000/$imageName sudo docker tag e2:5000/$imageName gcr.io/google_containers/$imageName sudo docker rmi e2:5000/$imageNamedone 批量拉取镜像并上传示例to_ali.sh123456789101112131415161718192021#!/usr/bin/env bashimages=( kube-proxy-amd64:v1.6.2 kube-controller-manager-amd64:v1.6.2 kube-apiserver-amd64:v1.6.2 kube-scheduler-amd64:v1.6.2 kubernetes-dashboard-amd64:v1.6.0 k8s-dns-sidecar-amd64:1.14.1 k8s-dns-kube-dns-amd64:1.14.1 k8s-dns-dnsmasq-nanny-amd64:1.14.1 etcd-amd64:3.0.17 pause-amd64:3.0)for imageName in $&#123;images[@]&#125; ; do sudo docker pull gcr.io/google_containers/$imageName sudo docker tag gcr.io/google_containers/$imageName registry.cn-shenzhen.aliyuncs.com/qhzxc0015/$imageName sudo docker push registry.cn-shenzhen.aliyuncs.com/qhzxc0015/$imageNamedonequay.io/coreos/flannel:v0.7.0-amd64sudo docker tag quay.io/coreos/flannel:v0.7.0-amd64 registry.cn-shenzhen.aliyuncs.com/qhzxc0015/flannel:v0.7.0-amd64sudo docker push registry.cn-shenzhen.aliyuncs.com/qhzxc0015/flannel:v0.7.0-amd64","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://qhzxc0015.com/tags/Docker/"},{"name":"Harbor","slug":"Harbor","permalink":"http://qhzxc0015.com/tags/Harbor/"}]},{"title":"Kubernetes DNS服务的安装与配置","slug":"07. kubeDNS","date":"2017-08-15T02:45:04.000Z","updated":"2017-08-29T08:34:00.271Z","comments":true,"path":"2017/08/15/07. kubeDNS.html","link":"","permalink":"http://qhzxc0015.com/2017/08/15/07. kubeDNS.html","excerpt":"Kubernetes的DNS服务是基于SkyDNS实现的，同时又需要和API Server紧密沟通，它的基本工作方式是通过API Server监视服务创建，一旦有新的服务创建就通知SkyDNS创建一条域名解析记录。沟通API Server和SkyDNS的工作都是由Kube2Sky完成的，Kube2sky和Skydns都需要使用ETCD实现共享配置和服务发现。","text":"Kubernetes的DNS服务是基于SkyDNS实现的，同时又需要和API Server紧密沟通，它的基本工作方式是通过API Server监视服务创建，一旦有新的服务创建就通知SkyDNS创建一条域名解析记录。沟通API Server和SkyDNS的工作都是由Kube2Sky完成的，Kube2sky和Skydns都需要使用ETCD实现共享配置和服务发现。 参考文献： Kubernetes DNS服务的安装与配置 Kubernetes技术分析之DNS 部署kubernetes dns服务 Kubernetes DNS服务搭建指南 Kubernetes提供的虚拟DNS服务名为skydns，由四个组件组成： etcd：DNS存储 kube2sky：强Kubernetes Master中的service（服务）注册到etcd。 skyDNS：提供DNS域名解析服务。 healthz：提供对skydns服务的健康检查。 创建两个配置文件skydns-rc.yaml和skydns-svc.yaml： rc.yaml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596apiVersion: v1kind: ReplicationControllermetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns version: v12 kubernetes.io/cluster-service: &quot;true&quot;spec: replicas: 1 selector: k8s-app: kube-dns version: v12 template: metadata: labels: k8s-app: kube-dns version: v12 kubernetes.io/cluster-service: &quot;true&quot; spec: containers: - name: etcd image: registry.cn-hangzhou.aliyuncs.com/kube_containers/etcd-amd64:3.0.17 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 100m memory: 50Mi command: - /usr/local/bin/etcd - --data-dir - /tmp/data - --listen-client-urls - http://127.0.0.1:2379,http://127.0.0.1:4001 - --advertise-client-urls - http://127.0.0.1:2379,http://127.0.0.1:4001 - --initial-cluster-token - skydns-etcd volumeMounts: - name: etcd-storage mountPath: /tmp/data - name: kube2sky image: gcr.io/google_containers/kube2sky-amd64:1.15 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 100m memory: 50Mi args: - --kube-master-url=http://202.193.74.179:8080 - --domain=cluster.local - name: skydns image: gcr.io/google_containers/skydns-amd64:v1.0 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 100m memory: 50Mi args: - -machines=http://127.0.0.1:4001 - -addr=0.0.0.0:53 - -ns-rotate=false - -domain=cluster.local ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - name: healthz image: gcr.io/google_containers/exechealthz-amd64:1.2 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi args: - -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 &gt;/dev/null - -port=8080 ports: - containerPort: 8080 protocol: TCP volumes: - name: etcd-storage emptyDir: &#123;&#125; dnsPolicy: Default svc.yaml: 1234567891011121314151617181920apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: default labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; kubernetes.io/name: &quot;KubeDNS&quot;spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://qhzxc0015.com/tags/k8s/"},{"name":"kubeDNS","slug":"kubeDNS","permalink":"http://qhzxc0015.com/tags/kubeDNS/"}]},{"title":"Kubernetes 部署","slug":"03. k8s 1.5.2","date":"2017-07-27T07:19:06.000Z","updated":"2017-11-29T13:30:13.135Z","comments":true,"path":"2017/07/27/03. k8s 1.5.2.html","link":"","permalink":"http://qhzxc0015.com/2017/07/27/03. k8s 1.5.2.html","excerpt":"本部署方法通过yum来进行自动安装,k8s版本并不是最新版,此方法简单,高效,操作性强,非常好","text":"本部署方法通过yum来进行自动安装,k8s版本并不是最新版,此方法简单,高效,操作性强,非常好 本部署为1.5.2版本:部署参考;可以根据具体需要进行版本升级:升级参考 1. 前期工作 安装环境 IP NAME 组件 OS 202.193.74.179 Master kube-apiserver,kube-scheduler,kube-controller-manager,etcd Centos 7.2 202.193.75.34 Node kube-proxy kubelet flannel Centos 7.2 202.193.75.11 Node kube-proxy kubelet flannel Centos 7.2 关闭防火墙 12sudo systemctl stop firewalld sudo systemctl disable firewalld 2. 部署环境Master 安装etcd和kubernetessudo yum -y install etcd kubernetes-master 配置文件1.修改/etc/etcd/etcd.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344# [member]ETCD_NAME=defaultETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ETCD_WAL_DIR=&quot;&quot;#ETCD_SNAPSHOT_COUNT=&quot;10000&quot;#ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;#ETCD_ELECTION_TIMEOUT=&quot;1000&quot;#ETCD_LISTEN_PEER_URLS=&quot;http://localhost:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://0.0.0.0:2379&quot;#ETCD_MAX_SNAPSHOTS=&quot;5&quot;#ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;##[cluster]#ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://localhost:2380&quot;# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;#ETCD_INITIAL_CLUSTER=&quot;default=http://localhost:2380&quot;#ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://localhost:2379&quot;#ETCD_DISCOVERY=&quot;&quot;#ETCD_DISCOVERY_SRV=&quot;&quot;#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;#ETCD_DISCOVERY_PROXY=&quot;&quot;#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;##[proxy]#ETCD_PROXY=&quot;off&quot;#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;##[security]#ETCD_CERT_FILE=&quot;&quot;#ETCD_KEY_FILE=&quot;&quot;#ETCD_CLIENT_CERT_AUTH=&quot;false&quot;#ETCD_TRUSTED_CA_FILE=&quot;&quot;#ETCD_AUTO_TLS=&quot;false&quot;#ETCD_PEER_CERT_FILE=&quot;&quot;#ETCD_PEER_KEY_FILE=&quot;&quot;#ETCD_PEER_CLIENT_CERT_AUTH=&quot;false&quot; 2.修改/etc/kubernetes/apiserver 1234567891011121314151617181920212223242526#### kubernetes system config## The following values are used to configure the kube-apiserver## The address on the local server to listen to.KUBE_API_ADDRESS=&quot;--insecure-bind-address=0.0.0.0&quot;# The port on the local server to listen on.# KUBE_API_PORT=&quot;--port=8080&quot;# Port minions listen on# KUBELET_PORT=&quot;--kubelet-port=10250&quot;# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=http://127.0.0.1:2379&quot;# Address range to use for servicesKUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;# default admission control policiesKUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;# Add your own!KUBE_API_ARGS=&quot;&quot; 3.修改/etc/kubernetes/controller-manager 1234567#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=&quot;--node-monitor-grace-period=10s --pod-eviction-timeout=10s&quot; 4.修改/etc/kubernetes/config 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://202.193.74.179:8080&quot; 启动服务开机自启sudo systemctl enable etcd kube-apiserver kube-scheduler kube-controller-manager启动服务sudo systemctl start etcd kube-apiserver kube-scheduler kube-controller-manager 配置etcd中的网络node节点的flannel会拉取这里的配置etcdctl mk /coreos.com/network/config &#39;{&quot;Network&quot;:&quot;172.17.0.0/16&quot;}&#39; Node(minions) 安装kubernetes-node和 flannel(会自动安装docker)sudo yum -y install kubernetes-node flannel 配置文件1.修改/etc/kubernetes/config 1234567891011121314151617181920212223#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot;KUBE_MASTER=&quot;--master=http://202.193.74.179:8080&quot; 2.修改/etc/kubernetes/kubelet (注意修改每个node的IP) 1234567891011121314151617181920#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)KUBELET_ADDRESS=&quot;--address=127.0.0.1&quot;# The port for the info server to serve on# KUBELET_PORT=&quot;--port=10250&quot;# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=&quot;--hostname-override=202.193.75.11&quot;# location of the api-serverKUBELET_API_SERVER=&quot;--api-servers=http://202.193.74.179:8080&quot;# pod infrastructure containerKUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&quot;# Add your own!KUBELET_ARGS=&quot;--pod-infra-container-image=kubernetes/pause&quot; 3.修改/etc/sysconfig/flanneld 123456789101112# Flanneld configuration options # etcd url location. Point this to the server where etcd runsFLANNEL_ETCD_ENDPOINTS=&quot;http://202.193.74.179:2379&quot;# etcd config key. This is the configuration key that flannel queries# For address range assignment#FLANNEL_ETCD_PREFIX=&quot;/atomic.io/network&quot;FLANNEL_ETCD_PREFIX=&quot;/coreos.com/network&quot;# Any additional options that you want to passFLANNEL_OPTIONS=&quot; -iface=ens33&quot; 其中FLANNEL_OPTIONS=&quot; -iface=eth0&quot;的eth0是网卡名称 启动服务sudo systemctl restart flanneld dockersudo systemctl start kubelet kube-proxysudo systemctl enable flanneld kubelet kube-proxy 3. 验证在master上1234[root@179 centos]# kubectl get nodesNAME STATUS AGE202.193.75.11 Ready 29m202.193.75.34 Ready 6m 4. 附录Error 1: 12[centos@179 message_board]$ kubectl get podsNo resources found. 解决方法:参考链接,认证问题,可以跳过认证方法:12#vim /etc/kubernetes/apiserverKUBE_ADMISSION_CONTROL=&quot;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot; 5. 添加节点master不变，node修改： Node(minions)中的2.修改/etc/kubernetes/kubelet(注意修改每个node的IP) –IP地址 Node(minions)中的3.修改/etc/sysconfig/flanneld –网卡修改 重启服务sudo systemctl restart flanneld dockersudo systemctl start kubelet kube-proxysudo systemctl enable flanneld kubelet kube-proxy","categories":[],"tags":[{"name":"技术","slug":"技术","permalink":"http://qhzxc0015.com/tags/技术/"},{"name":"k8s","slug":"k8s","permalink":"http://qhzxc0015.com/tags/k8s/"}]},{"title":"Markdown介绍","slug":"02. md介绍","date":"2017-07-20T13:55:06.000Z","updated":"2017-09-09T06:51:11.719Z","comments":true,"path":"2017/07/20/02. md介绍.html","link":"","permalink":"http://qhzxc0015.com/2017/07/20/02. md介绍.html","excerpt":"Markdown 语法简单，记忆负担小，纯文本，流畅书写，无违和感，各种实时渲染效果,所以被刚广泛使用","text":"Markdown 语法简单，记忆负担小，纯文本，流畅书写，无违和感，各种实时渲染效果,所以被刚广泛使用 优点 语法简单，记忆负担小，纯文本，流畅书写，无违和感，各种实时渲染效果。 markdown是为那些需要经常码东西并且进行文字排版的、对码字手速和排版顺畅度有要求的人群设计的，他们希望用键盘把文字内容啪啪啪地打出来后就已经排版好了，最好从头到尾都不要使用鼠标。这些人包括经常需要写文档的码农、博客写手、网站小编、出版业人士等等。 通常情况下，网络上需要进行大量文字输入的地方都可以通过所见即所得的方式排版 再强调一下实时的渲染效果。无论新手老手写 Markdown 的时候最担心的是不知道自己写的对不对，最后渲染效果是不是准确无误。就好象写完代码你一定要自己测试一下才放心，写 Markdown 就是在写代码，实时的渲染效果就是在做测试，这就是为什么网络上有那么多左右两屏带实时渲染的 Markdown 工具的原因（另一个原因是宽屏）1 公式,[^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\\sum_{i=1}^n a_i=0$， 访问 MathJax $$ x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ $$\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}dt\\,.$$ $$ \\begin{matrix} 1 &amp; x &amp; x^2 \\ 1 &amp; y &amp; y^2 \\ 1 &amp; z &amp; z^2 \\ \\end{matrix}$$ $$ \\left[ \\begin{array}{cc|c} 1&amp;2&amp;3\\ 4&amp;5&amp;6 \\end{array}\\right] $$ \\begin{align}\\sqrt{37} &amp; = \\sqrt{\\frac{73^2-1}{12^2}} \\ &amp; = \\sqrt{\\frac{73^2}{12^2}\\cdot\\frac{73^2-1}{73^2}} \\ &amp; = \\sqrt{\\frac{73^2}{12^2}}\\sqrt{\\frac{73^2-1}{73^2}} \\ &amp; = \\frac{73}{12}\\sqrt{1 - \\frac{1}{73^2}} \\ &amp; \\approx \\frac{73}{12}\\left(1 - \\frac{1}{2\\cdot73^2}\\right)\\end{align} $ \\newcommand{\\SES}[3]{ 0 \\to #1 \\to #2 \\to #3 \\to 0 } $ 表格| 1 | 2 | 3 | 4 || —- | —: | :–: | —- || 1 | 1 | 1 | 1 | Item Value Computer \\$1600 Phone \\$12 Pipe \\$1 1.这个是脚注 ↩","categories":[],"tags":[{"name":"技术","slug":"技术","permalink":"http://qhzxc0015.com/tags/技术/"},{"name":"Markdown","slug":"Markdown","permalink":"http://qhzxc0015.com/tags/Markdown/"}]},{"title":"hexo基本配置","slug":"01. hexo","date":"2017-07-19T13:55:06.000Z","updated":"2017-08-09T08:56:16.305Z","comments":true,"path":"2017/07/19/01. hexo.html","link":"","permalink":"http://qhzxc0015.com/2017/07/19/01. hexo.html","excerpt":"","text":"这里不错吧,是吧,是吧,来来来,看一下hexo啦~ 常用命令 1234567891011121314hexo clean #清除PUBLIC和编译文件hexo generate #编译网站目录hexo deploy #同步到GIT 或者CODINGnpm install &lt;plugin-name&gt; --save #安装npm update #升级npm uninstall &lt;plugin-name&gt; #卸载hexo new”postName” #新建文章 #存放在主目录的source下的POST目录下hexo new page”pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 主目录 12345678910├── .deploy #需要部署的文件├── node_modules #Hexo插件├── public #生成的静态网页文件├── scaffolds #模板├── source #博客正文和其他源文件, 404 favicon CNAME 等都应该放在这里| ├── _drafts #草稿| └── _posts #文章├── themes #主题├── _config.yml #全局配置文件└── package.json 主题目录 123456789101112131415161718├── languages #国际化| ├── default.yml #默认| └── zh-CN.yml #中文├── layout #布局| ├── _partial #局部的布局| └── _widget #小挂件的布局├── script #js脚本├── source #源代码文件| ├── css #CSS| | ├── _base #基础CSS| | ├── _partial #局部CSS| | ├── fonts #字体| | ├── images #图片| | └── style.styl #style.css| ├── fancybox #fancybox| └── js #js├── _config.yml #主题配置文件└── README.md #主题介绍 以上目录中常用的有: scaffolds source themes config.yml 根配置文件:_config.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Site #站点信息title: lmintlcx #标题subtitle: 做人不卖萌跟咸鱼有什么区别 #副标题description: lmintlcx lm lcx blog #描述author: lmintlcx #作者language: zh-Hans #语言timezone: Asia/Shanghai #时区# URL #链接格式url: http://joryhe.coding.me/ #网址root: / #根目录permalink: post/:title.html #文章的链接格式permalink_defaults:# Directory #目录source_dir: source #源文件public_dir: public #生成的网页文件tag_dir: tags #标签archive_dir: archives #归档category_dir: categories #分类code_dir: downloads/codei18n_dir: :lang #国际化skip_render:# Writing #写作new_post_name: :title.md #新文章标题default_layout: post #默认模板titlecase: false #标题转换成大写external_link: true #新标签页里打开连接filename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: #语法高亮 enable: true line_number: false #显示行号 auto_detect: true tab_replace:# Category &amp; Tag #分类和标签default_category: uncategorized #默认分类category_map:tag_map:# Date / Time format #日期时间格式date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination #分页per_page: 20 #每页文章数, 设置成 0 禁用分页pagination_dir: page# Extensions #插件和主题## 插件: http://hexo.io/plugins/## 主题: http://hexo.io/themes/theme: next# Deployment #部署, joryhe是我的用户名, 同时发布GitHub deploy: type: git repo: github: github: git@github.com:joryhe/joryhe.github.io.git,master# Disqus #Disqus评论系统disqus_shortname: plugins: #插件，例如生成 RSS 和站点地图的- hexo-generator-feed- hexo-generator-sitemap 主题配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677menu: #菜单 home: / #首页 archives: /archives #归档 about: /about #关于 #commonweal: /404.html #公益404 #tags: /tags #标签 #categories: /categories #分类## 经典介绍配置# 小图标favicon: /favicon.ico# 默认关键词keywords: # 留空使用默认的, false 禁用, 也可以写指定的地址rss:# Icon fonts# default | linecons | fifty-shades | feathericon_font: default# 代码高亮主题 https://github.com/chriskempson/tomorrow-theme# normal | night | night eighties | night blue | night brighthighlight_theme: normal# MathJax Support #数学公式mathjax: true# Schemes #启用主题中的主题Mistscheme: Mist# 侧边栏# - post 只在文章页面显示# - always 所有页面显示# - hide 隐藏sidebar: always# 自动滚动到&quot;阅读更多&quot;标记的下面scroll_to_more: true# 自动给目录添加序号toc_list_number: true# 自动截取摘要auto_excerpt: enable: false length: 150# Lato 字体use_font_lato: true# Make duoshuo show UA# user_id must NOT be null when admin_enable is true!# you can visit http://dev.duoshuo.com get duoshuo user id.duoshuo_info: ua_enable: true admin_enable: false user_id: 0 #admin_nickname: ROOT## DO NOT EDIT THE FOLLOWING SETTINGS## UNLESS YOU KNOW WHAT YOU ARE DOING# 动画use_motion: true# Fancybox 看图插件fancybox: true# Static filesvendors: vendorscss: cssjs: jsimages: images# Theme versionversion: 0.4.5.1","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://qhzxc0015.com/tags/hexo/"}]}]}